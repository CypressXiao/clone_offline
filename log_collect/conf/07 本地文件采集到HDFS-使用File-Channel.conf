#  使用filechannel 提成数据的可高性
#  1 定义一个name为a1的agent  三个组件的别名  分别是 s1  c1  k1
a1.sources = s1
a1.channels = c1
a1.sinks = k1

#  数据在FLume内部以Event的方式传输
#   Event =  Body(数据) + Header(属性)
#
#2  source    TailDir
#  定义source类型
a1.sources.s1.type  = TAILDIR
a1.sources.s1.filegroups = g1 g2 g3
a1.sources.s1.filegroups.g1 = /doe/data/applogs/doe_app_events.log
a1.sources.s1.filegroups.g2 = /doe/data/wxlogs/wx.*
a1.sources.s1.filegroups.g3 = /doe/data/weblogs/web.*
# 在不同的日志文件中的Event Header中添加数据类别
a1.sources.s1.headers.g1.logType = app_log
a1.sources.s1.headers.g2.logType = wx_log
a1.sources.s1.headers.g3.logType = web_log
# 修改记录采集数据便宜量文件的位置
a1.sources.s1.positionFile = /opt/flume_data/positions/taildir_position.json


# 配置flume内置的拦截器   host主机拦截器  会在事件的头信息中添加主机信息
a1.sources.s1.interceptors = i1
# 自定义拦截器  抽取日志中的时间
a1.sources.s1.interceptors.i1.type = cn.doitedu.flume.interceptor.ExtractTimeInterceptor$Builder
#  出入的log_dt 作为拦截器中Header的key  put("log_dt" , 解析的日期)
a1.sources.s1.interceptors.i1.x = log_dt
a1.sources.s1.channels = c1


# 使用filechannel  提升采集数据的可靠性 提升数据的安全性  牺牲一定的性能
a1.channels.c1.type = file
# 为了保证数据的安全 引入checkpoint机制  进一步保证采集的数据不会丢失
a1.channels.c1.checkpointDir =/opt/flume_data/checkpoint
# 存储Event数据到指定的文件夹下
a1.channels.c1.dataDirs =/opt/flume_data/data

#sink        HDFS
a1.sinks.k1.channel = c1
a1.sinks.k1.type = hdfs
# 在sink时 可以获取自定义拦截器中  添加的日志时间学习
a1.sinks.k1.hdfs.path = hdfs://doitedu01:8020/doe/data/%{logType}/%{log_dt}
# 生成的结果文件可以指定文件的前缀和后缀
a1.sinks.k1.hdfs.filePrefix = doit32_
# 后缀根据文件的类型
a1.sinks.k1.hdfs.fileSuffix = .log

# 默认数据大量的小文件  , 改变输出策略   ,128M滚动一个输出文件
# 默认是30秒滚动生成一个文件  0 禁用时间滚动生成文件
a1.sinks.k1.hdfs.rollInterval = 0
# 默认是1K滚动生成一个文件   128M生成一个文件
a1.sinks.k1.hdfs.rollSize = 134217728
# 默认是10条Event生成一个文件  0 代表禁用以事件个数生成文件的策略
a1.sinks.k1.hdfs.rollCount = 0
#  没有达到文件滚动的时机, 会占用一个临时文件 .tmp ,如果数据长时间的没有生成 , 占用资源 关闭临时文件
#  在指定的时间内 没有采集到新的数据  临时文件会关闭
a1.sinks.k1.hdfs.idleTimeout = 10
# 将数据sink到HDFS可以获取到数据的头信息 , hdfs sink  可以使用当前机器的时间
# a1.sinks.k1.hdfs.useLocalTimeStamp = true