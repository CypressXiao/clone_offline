#  采集本地多个目录下的多个文件到HDFS中
#  根据不同的日志类别  将采集的数据保存在HDFS不同的目录下
#  主要是配置source

#  1 定义一个name为a1的agent  三个组件的别名  分别是 s1  c1  k1
a1.sources = s1
a1.channels = c1
a1.sinks = k1

#  数据在FLume内部以Event的方式传输
#   Event =  Body(数据) + Header(属性)
#
#2  source    TailDir
#  定义source类型
a1.sources.s1.type  = TAILDIR
a1.sources.s1.filegroups = g1 g2 g3
a1.sources.s1.filegroups.g1 = /doe/data/applogs/doe_app_events.log
a1.sources.s1.filegroups.g2 = /doe/data/wxlogs/wx.*
a1.sources.s1.filegroups.g3 = /doe/data/weblogs/web.*
# 在不同的日志文件中的Event Header中添加数据类别
a1.sources.s1.headers.g1.logType = app_log
a1.sources.s1.headers.g2.logType = wx_log
a1.sources.s1.headers.g3.logType = web_log
# 修改记录采集数据便宜量文件的位置
a1.sources.s1.positionFile = /opt/flume_data/positions/taildir_position.json
a1.sources.s1.channels = c1

# 获取到Event的Header  [Map]中的数据

#channel    memory
a1.channels.c1.type = memory
#sink        HDFS
a1.sinks.k1.channel = c1
a1.sinks.k1.type = hdfs
# 在sink时 可以获取 EventHeader中的数据
a1.sinks.k1.hdfs.path = hdfs://doitedu01:8020/doe/data/%{logType}/
# 默认数据大量的小文件  , 改变输出策略   ,128M滚动一个输出文件
# 默认是30秒滚动生成一个文件  0 禁用时间滚动生成文件
a1.sinks.k1.hdfs.rollInterval = 0
# 默认是1K滚动生成一个文件   128M生成一个文件
a1.sinks.k1.hdfs.rollSize = 134217728
# 默认是10条Event生成一个文件  0 代表禁用以事件个数生成文件的策略
a1.sinks.k1.hdfs.rollCount = 0
#  没有达到文件滚动的时机, 会占用一个临时文件 .tmp ,如果数据长时间的没有生成 , 占用资源 关闭临时文件
#  在指定的时间内 没有采集到新的数据  临时文件会关闭
a1.sinks.k1.hdfs.idleTimeout = 10